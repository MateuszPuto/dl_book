% -*- root: ../main.tex -*-
\chapter{Gry i więcej}
\label{chap:games}

Pokryliśmy w tym momencie szeroki zakres tematów, zaczynając na sieciach neuronowych i optymalizowaniu ich działania, co daje nam moc znajdywania relacji między strumieniami danych. Patrzyliśmy się na drzewa poszukiwać, które utrzymują w pamięci rozwiązania, których próbowaliśmy i pomagają nam eksplorować przestrzeń możliwości. Widzieliśmy także, w jaki sposób uczenie ze wzmocnieniem daje nam unifikujący pogląd na aktorów uczących się w rzeczywistych środowiskach i zobaczyliśmy problemy, które w nich napotykają. Wspomnieliśmy też czasami o historycznych wydarzeniach ważnych dla dziedziny sztucznej inteligencji. W tym rozdziale przeglądniemy kilka prac badawczych od firmy Deepmind, mając na uwadze to, czego nauczyliśmy się do tej pory. Po przeczytaniu tego rozdziału powinieneś być zapoznany, przynajmniej na wysokim poziomie z najnowszymi badaniami w tej dziedzinie. Pospekulujemy również na końcu tego rozdziału nad przyszłymi możliwymi podejściami do tak zwanej generalnej sztucznej inteligencji tj. \textbf{AGI}. Ta część tekstu jest może najważniejsza ze wszystkich. Budując na tym, co zostało powiedziane wcześniej, daje nietechnicznemu czytelnikowi wgląd w to, co dzieje się pod maską sztucznej inteligencji, uczenia maszynowego oraz głębokich sieci neuronowych. Odbiegając od tematu, zwrot głęboka sieć neuronowa (ang. deep neural network) znaczy tylko tyle że sieć ma więcej niż jedną warstwę neuronów. To tyle. Jednak ktoś niezaznajomiony z tematem mógłby być pod wrażeniem, kiedy usłyszałby o głębokich sieciach neuronowych. Mam nadzieję, że teraz rozumiesz głębiej, jak wygląda rzeczywistość i nie będziesz tak łatwo manipulowany przez medialne próby łapania twojej uwagi. Mamy także nadzieję, że niektóre obawy związane ze sztuczną inteligencją zostały przez nas rozwiane i możesz krytykować rozwiązania AI na wyższym poziomie zrozumienia. Mamy nadzieję, że będziesz w stanie wyjaśnić kilka rzeczy swoim znajomym. Ostatecznie mamy nadzieję, że będziesz w stanie podejmować lepsze decyzje, używając tej wiedzy. Czy to będzie poprzez dalsze pogłębianie wiedzy i tworzenie nowych rozwiązań, przygotowanie lepszego planu dla swojej firmy, inwestowanie w przedsięwzięcia związane z technologią, czy nawet decyzje dotyczące życia osobistego takie jak wybór platform społecznościowych, z których chcesz korzystać, ze względu na wykorzystywanie w nich tych metod. Jesteśmy głęboko przekonani, że większa wiedza może cię prowadzić do lepszego życia dla ciebie i innych. Popatrzmy teraz na obecny stan badań.

\section{AlphaGo (rok 2016)}

\textbf{AlphaGo} jest algorytmem, który został stworzony z myślą o pokonaniu gry planszowej, która nie poddawała się technikom sztucznej inteligencji przez najdłuższy czas, mowa tu o Go. W tamtym czasie naukowcy myśleli, że znaczący przełom w dziedzinie jest konieczny, aby osiągnąć ten cel oraz że jest on odległy o przynajmniej dekadę. W tym czasie brytyjska firma Deepmind przejęta przez Google ciężko pracowała, aby udowodnić coś przeciwnego. Sprawdzili oni najlepsze w tamtym czasie podejścia do tej gry, które używały MCTS wzmocnionego przez sieć zbioru zasad, która miała przewidywać ruchy, które zagrałby człowiek. Badzacze zaproponowali ich własny sposób, który miał używać sieci zbioru zasad i funkcji wartości połączonych z MCTS, a to wszystko wytrenowane w części przez uczenie ze wzmocnieniem. Jak pokazują w pracy na temat AlphaGo, gra Go ma bardzo dużą przestrzeń możliwości, w której trzeba wyszukiwać rozwiązanie. Jej szerokość $\boldsymbol{b}$ (ilość legalnych ruchów dla danej pozycji) wynosi około 250, a głębokość $\boldsymbol{d}$ (długość gry), która wynosi średnio 150. To jest dużo większe drzewo możliwości niż np. szachy, które mają $\boldsymbol{b}$ równe mniej więcej 35, a $\boldsymbol{d}$ równe około 80. Z tego właśnie powodu proste wyszukiwanie alfa-beta nie jest wystarczające, aby rozwiązać problem gry Go. Nie możemy popatrzeć się wystarczająco głęboko i nie mamy prostej funkcji wartości, tak jak w szachach gdzie każdej figurze możemy przypisać pewną wartość. W Go wartość każdego ruchu zależy głównie od innych ruchów, które zostały wcześniej wykonane. Żeby nauczyć AlphaGo, Deepmind wytrenował kilka sieci neuronowych. Na początku zebrali zbiór danych zawierający gry profesjonalistów. Następnie wytrenowali coś, co nazwali siecią ‘SL policy’ żeby przewidywała ruchy zrobione przez tych graczy. Ta sieć miała 13 warstw i trenowała na 30 milionach przykładów. Naukowcy wytrenowali dodatkowo mniejszą i szybszą sieć do wykonywania tego samego zadania. ‘SL policy’ udało się otrzymać wynik 57\% poprawnie przewidzianych ruchów, a mniejszej sieci udało się to w 24.2\% przypadków. Kolejno wytrenowano ‘SL policy’ sposobem uczenia przez wzmacnianie. Pozwolono sieci wybierać ruchy i na końcu gry, gdy znana jest nagroda, dokonywano uczenia ze wzmocnieniem. Użyta metoda opierała się o podobne mechanizmy jak te, na które patrzyliśmy tylko dla sieci zbioru zasad zamiast dla funkcji wartości. Wytrenowana w ten sposób sieć RL była w stanie pokonać sieć SL w 80\% przypadków. Ludzie pracujący nad projektem chcieli jednak aby użyć także funkcji wartości. Żeby otrzymać zbiór danych, pozwolili grać sieci RL wiele gier ze samą sobą aż do momentu ich zakończenia. Następnie wybierali niektóre pozycje spośród tego zbioru danych, żeby uniknąć nadmiernego dopasowania ze względu na korelacje pomiędzy pozycjami wewnątrz jednej gry. Funkcją błędu była MSE względem wyniku gry, który był równy +1 dla wygranej i -1 dla przegranej (w grze Go nie ma remisów). Architektura sieci symulującej funkcję wartości była podobna do sieci ‘SL policy’. Oczywista różnica, jaka między nimi występowała to fakt, że ‘SL policy’ zwracała dystrybucję prawdopodobieństwa a funkcja wartości jedną liczbę. Sieć ‘policy’ była przydatna w zmniejszaniu szerokości drzewa, a funkcja wartości w zmniejszaniu głębokości drzewa poszukiwań. W pracy na temat AlphaGo badacze zwracają uwagę, że funkcja wartości miała podobną siłę do sieci RL używającej rozwinięć Monte-Carlo, ale używała 15 000 razy mniej mocy obliczeniowej. Ostatecznie połączyli oni sieci neuronowe z drzewem poszukiwań, a mianowicie algorytmem MCTS wcześniej opisanym w rozdziale 4.6. Zamiast używać formuły UCB, wybierają oni węzeł za pomocą następującego równania:

\begin{equation}
a_t = argmax_a(Q(s, a), + u(s, a))
\end{equation}

\noindent Gdzie $\boldsymbol{s}$ jest stanem, $\boldsymbol{a}$ akcją, a $\boldsymbol{argmax_a}$ oznacza wybór $\boldsymbol{a}$ z największą wartością $\boldsymbol{Q + u}$ które są opisane poniżej. Wyrażenie $\boldsymbol{Q(s, a)}$ jest zdefiniowane jako:

\begin{equation}
Q(s, a) = 1/N(s, a) * \Sigma(1(s, a, i) * V(s))
\end{equation}

\noindent Gdzie $\boldsymbol{1(s, a, i)}$ jest funkcją charakterystyczną zbioru, która określa czy węzeł był odwiedzony podczas \textbf{i-tej} symulacji. $\boldsymbol{V(s)}$ jest funkcją wartości, a $\boldsymbol{N(s, a)}$ jest liczbą wizyt w danym węźle.\newline

\noindent Drugie wyrażenie w równaniu (6.1) jest zdefiniowane:

\begin{equation}
u(s, a) = c * P(s, a) * \sqrt{\sum_{}^{b}N(s, b) / N(s, a)}
\end{equation}

\noindent Gdzie $\boldsymbol{c}$ jest stałą eksploracji, $\boldsymbol{P(s, a)}$ jest wcześniejszym prawdopodobieństwem, które otrzymaliśmy za pomocą policy użytej na węźle nadrzędnym, i $\boldsymbol{N(s, b)}$ jest liczba wizyt do możliwych ruchów z naszego węzła, a $\boldsymbol{N(s, a)}$ jest liczbą wizyt w naszym węźle.\newline

\noindent Wartość $\boldsymbol{V(s)}$ z równania (6.2) jest kombinacją wartości wyjścia z sieci $\boldsymbol{v_\theta}$ oraz wartości rozwinięcia (ang. rollout) $\boldsymbol{z}$ które są połączone w następujący sposób:

\begin{equation}
V(s) = (1 - \lambda) * v_\theta + \lambda * z
\end{equation}

\noindent Gdzie $\boldsymbol{\lambda}$ jest parametrem mieszania.\newline

W równaniu (6.1) podobnie jak w MCTS $\boldsymbol{u}$ jest częścią równania odpowiadającą za eksplorację, natomiast $\boldsymbol{Q}$ jest częścią równania określającą wartość węzła. $\boldsymbol{Q}$ jest po prostu wartością $\boldsymbol{V}$ znormalizowaną przez ilość rozwinięć, tak żeby ich ilość nie wpływała na wynik. Jeśli chodzi o $\boldsymbol{u}$ (6.3) to, zamiast określać nasz brak wiedzy na temat węzła tylko przy pomocy ilości rozwinięć, używana do określenia tej potencjalnej wartości jest również wartość otrzymana z sieci zbioru zasad. To ona szybko, za jednym razem, określa wartość wszystkich ruchów na planszy. W równaniu (6.4) widzimy, że zamiast wykorzystywać samą wartość rozwinięcia, jak to miało miejsce w klasycznym MCTS, wykorzystywana jest również informacja na temat pozycji pozyskana z sieci funkcji wartości.\newline

Po fazie rozwinięcia wszystkie parametry i niektóre ukryte parametry muszą być przesłane do nadrzędnych węzłów. Nie będziemy tu wchodzić w szczegóły. Jedyną rzeczą, którą musimy teraz dodać, jest odpowiedź na pytanie, kiedy rozwijać węzeł. Ekspansja jest wykonywana, kiedy wartość wizyt w węzle $\boldsymbol{N(s, a)}$ jest większa niż pewna wartość $\boldsymbol{n_{threshold}}$. Jako najlepszy węzeł jest wybierany węzeł z największą ilością wizyt. Pokrótce to jest całość algorytmu AlphaGo. Został on użyty w Marcu 2016, żeby pokonać byłego mistrza świata Lee Sedola w serii pięciu meczów. Jedyna gra wygrana przez człowieka, gra czwarta, została wygrana dzięki specjalnej taktyce Sedola, która miała na celu skomplikowanie gry. Podczas tego wysiłku znalazł on ruch nazywany „boskim ruchem”. AlphaGo oceniała go jako bardzo mało prawdopodobny. Prawdopodobnie z powodu tego błędu komputer zagrał poniżej oczekiwań w kolejnych kilku posunięciach, tracąc w efekcie wiele punktów na rzecz człowieka i w konsekwencji przegrywając całą grę. Później zespół AlphaGo znalazł i poprawił błąd, który powodował dziwne zachowanie.

\section{AlphaZero (rok 2017)}

W 2017 roku zespół Deepmind wziął się za rozwiązywanie szachów i gry Shogi używając podobnych technik do tych użytych w AlphaGo, tym razem jednak miano nie wykorzystywać wiedzy ludzi do nauki, a zamiast tego programy miały się nauczyć grać od podstaw na ponadludzkim poziomie. Normalne silniki szachowe używają techniki wyszukiwania alfa-beta wraz z bardzo dokładnie ręcznie dopasowaną funkcją wartości, książką otwarć i końcówek oraz korzystają z wielu heurystyk, które poprawiają działanie systemu w wielu małych okolicznościach. AlphaZero przyniosła zupełnie nowy sposób myślenia o problemie, pozwalając na stworzenie silnika szachowego Lc0, który jest w stanie konkurować z najlepszym klasycznym silnikiem szachowym, mianowicie Stockfishem. Leela Zero była w stanie osiągnąć to w bardzo krótkim czasie rozwoju i w przyszłości będzie prawdopodobnie w stanie przekonywająco pokonać Stockfisha. \textbf{AlphaZero} korzysta z architektury pod wieloma względami podobnej do programu AlphaGo. Używa sieci ‘policy’, sieci wartości oraz algorytmu MCTS. Największą różnicą pomiędzy nimi jest metoda trenowania obu modeli. Kiedy AlphaGo było trenowane przy użyciu gier pochodzących od ludzi, AlphaZero uczy się tabula rasa, znając na początku tylko zasady gry. Wyszukiwanie jest wg autorów zaimplementowane podobnie jak w AlphaGo. Żeby stać się lepszym, AlphaZero grało przeciwko sobie i było wzmacniane -1 dla przegranej, 0 dla remisu i +1 dla wygranej. Parametry sieci były uczone przy użyciu MSE + Cross entropy (inny typ funkcji błędu) + regularyzacja $L^2$, która została opisana w rozdziale 3.6. Najbardziej interesującym pomysłem jest to, że parametry w sieci zbioru zasad są aktualizowane tak, aby zbliżyć je do prawdopodobieństwa po użyciu wyszukiwania, więc w pewnym sensie sieć zbioru zasad stara się przewidzieć co stanie się podczas wyszukiwania. Dzięki temu przy wykorzystaniu, sieć zbioru zasad jest w stanie zredukować sprawdzane przypadki do tylko do tych najbardziej obiecujących. AlphaZero wyszukuje 80 tys. pozycji na sekundę w porównaniu do 70 milionów, które sprawdza Stockfish. Jednak mimo tego AlphaZero była w stanie pokonać Stockfisha w meczu 1000 gier. Rezultat wyniósł 155 zwycięstw dla AlphaZero, 6 dla Stockfisha, reszta to remisy.

\section{MuZero (rok 2019)}

Architektura \textbf{MuZero} jest pod pewnymi względami podobna do AlphaZero, ale w niektórych znacząco się różni. MuZero używa podobnej wersji MCTS. Łączy też ‘policy’ i funkcję wartości w jedną sieć z dwoma wyjściami.

\begin{equation}
(6.3.1) p, v = f(s)
\end{equation}

\noindent Gdzie $\boldsymbol{p}$ jest policy, $\boldsymbol{v}$ funkcją wartości, $\boldsymbol{f}$ siecią neuronową, a $\boldsymbol{s}$ jej wejściem.
Największą zmianą są nowe funkcje dynamiki i reprezentacji. Funkcja dynamiki jest używana do przewidywania nagrody w następnym kroku. Stan jest ustawiony na równy funkcji reprezentacji.

\begin{equation}
(6.3.2) s_0 = h(o\_1, o\_2, ..., o\_t)
\end{equation}

\noindent Zamiast używać danych jako wejścia, używana jest funkcja reprezentacji $\boldsymbol{h}$ która bierze jako wejście dane wejściowe $\boldsymbol{o\_t}$ z poprzednich $\boldsymbol{t}$ kroków.\newline

To jest najbardziej ekscytująca idea w MuZero. Oznacza ona właściwie, że możemy wykonywać wyszukiwanie w sytuacjach, w których było to poprzednio niemożliwe, takich jak np. gry Atari. W grach Atari nie posiadamy zasad pozwalających nam na generowanie następnej pozycji z pozycji obecnej i wybranego ruchu. To jest miejsce, gdzie funkcja reprezentacji jest wykorzystywana. Raczej niż polegać na zasadach do generowania przyszłych pozycji, wykorzystujemy do tego celu funkcję reprezentacji. To pozwala na wyszukiwanie w sytuacjach, w których generowanie drzewa poszukiwań było poprzednio niemożliwym. Żeby docenić to podejście, pomyślmy o sytuacji, w której gramy np. w grę komputerową. Nie znamy nigdy dokładnych zasad, na których opiera się ta gra. Jedyne co wiemy na początku to kilka komend kontrolnych, które podał nam twórca gry. Reszty musimy się domyślić, opierając się na naszej wiedzy o prawdziwym świecie, a może także o innych grach, które napotkaliśmy wcześniej. Tę funkcję spełnia funkcja reprezentacji. Mając taką funkcję reprezentacji, możemy przeprowadzać planowanie za pomocą drzewa poszukiwań w sutuacji, w której poprzednio nie było to możliwe. Możemy np. wyobrazić sobie, że jeśli strzelimy do strażnika, to inni strażnicy zostaną poinformowani o czyhającym na nich zagrożeniu. Tak więc wybieramy inną ścieżkę dostępną na drzewie poszukiwań, które zostało wygenerowanie dzięki funkcji reprezentacji. To podejście osiągnęło nowy rekord w rozwiązywaniu gier Atari, jak jest to wyjaśnione w pracy na temat MuZero. Jedynym pytaniem pozostaje jak wytrenować taki model. Jak możemy przeczytać, wszystkie parametry funkcji reprezentacji, dynamiki i predykcji są trenowane razem. Błąd z wszystkich sieci jest sumowany i dodawana jest regularyzacja $L^2$. MuZero używa drzewa poszukiwań MCTS, używając funkcji predykcji, reprezentacji i dynamiki. Następnie po $\boldsymbol{n}$ = 800 symulacji, ruch jest wybierany. Ta procedura jest wykonywana przez \textbf{K} kroków gry. Następnie, żeby uczenie było możliwe, najbliższa gałąź MCTS jest porównywana do prawdziwej gry. To porównanie daje nam dane treningowe dla funkcji błędu. To porównywanie gałęzi jest kolejną ekscytującą ideą w architekturze MuZero. Sprawia to, że może się ono uczyć nie tylko na bezpośrednio wykonanym ruchu, ale na całej sekwencji zasymulowanych ruchów. Porównując to, co zostało zasymulowane, z tym, co stało się naprawdę, uzyskujemy błąd. Ten błąd z każdego z przykładów wykorzystujemy do trenowania sieci. To podejście sprawia, że możemy uzyskać dużo więcej przykładów testowych. MuZero wydaje się niezwykle ciekawe, ponieważ zdaje się, że może być użyte nie tylko do gier ze znanymi zasadami, ale także do każdego innego problemu uczenia ze wzmocnieniem. MuZero wygląda jak spojrzenie na to, jak systemy AI będą wyglądać w przyszłości.

\section{AGI}

\textbf{Generalna sztuczna inteligencja} (ang. artificial general intelligence, AGI) jest ideą, którą mówi, że jest możliwym stworzyć program, który będzie w stanie rozwiązać dowolny problem, który mu zaprezentujemy. Część opisanego postępu zdaje się poruszać w stronę większej ogólności jak na przykład AlphaZero, które można wytrenować, aby grało w kilka różnych gier. Jest jednak również prawdą, że wielu naukowców ma duże wątpliwości na temat możliwości osiągnięcia AGI w bliskiej przyszłości. AI posunęło się na wielu frontach w czasie ostatnich kilku dekad. Poprawiliśmy metody optymalizacji, żeby być w stanie lepiej odpowiadać na intuicyjne pytania. Poprawiliśmy algorytmy wyszukiwania tak, aby mogły lepiej pracować z intuicjami sieci neuronowych. Zbudowaliśmy także zaawansowaną formalną strukturę, dzięki której możemy myśleć o problemie uczenia ze wzmocnieniem. Nadal jest jedna duża rzecz, która zdaje się nam uciekać. Jest nią język. Każdy człowiek na Ziemi komunikuje się, używając pewnego rodzaju języka, i mamy trudności z wyobrażeniem sobie jak wyglądałoby nasze życie bez niego. To jest według autora dosyć silny argument za koniecznością języka w naszych systemach. Czym w ogóle jest \textbf{język}? Język jest zbiorem znaków połączonych z pewnymi znaczeniami. Możemy używać tych znaków, żeby opisywać sytuacje, przewidywać kolejno następujące znaki i do generowania odpowiedzi na pytania skonstruowane za pomocą tych znaków. Jak jednak wcześniej widzieliśmy, idee o wielkiej mocy muszą być dodane do potężnych struktur, żeby były szeroko aplikowalne. Zobaczmy, jak te symboliczne pomysły mogłyby być użyte w połączeniu z sieciami neuronowymi, żeby dać najlepsze rezultaty: Czy pamiętasz, jak silnik szachowy wyszukiwał miliony przykładów na sekundę, ale rozwiązania oparte o sieci neuronowe wykonywały dużo mniej takich wyszukiwań, osiągając mimo tego dobre rezultaty ze względu na użycie sieci neuronowych do oceny sytuacji? Możemy myśleć o drzewach poszukiwań jako, w pewnym sensie, bardzo podobnych do języka. Ostatecznie przecież w wielu modelach języka zdanie może być zapisane jako swego rodzaju drzewo. Jeśli weźmiemy ten pomysł krok dalej, to możemy stwierdzić, że te pomysły są dokładnie takie same. To znaczyłoby, że ludzie wykonują nawet mniej wyszukiwania w porównaniu do MuZero, jednocześnie przewyższając je w większości dziedzin. Znaczyłoby to także że język jest rodzajem algorytmu poszukiwań kierującym nas w wyborze następnego węzła do rozwinięcia. Jednak język ma pewne właściwości, których naszym systemom nadal brakuje. Jak możemy opisać język jako algorytm, tak jak robiliśmy to wcześniej dla różnych metod? Na początku stwierdzilibyśmy autorytatywnie, że słowa mogą być reprezentowane jako pewne punkty w $\boldsymbol{n}$ wymiarowej przestrzeni. To może się nie wydawać intuicyjne, ponieważ słowa wydają się dyskretnymi bytami, ale może to wynikać z używania tylko pewnych części dostępnej przestrzeni. Następną rzeczą, którą byśmy wprowadzili, jest to, że każdy węzeł powinien posiadać korespondujące do niego jedno słowo, które go opisuje. Oczywiście możemy dać listę słów albo opis sytuacji za pomocą całego zdania, albo paragrafu, ale może wyraża to tylko pierwsze słowo z większą dokładnością. Słowa są też połączone ze sobą. Wszystkie takie relacje mogłyby teoretycznie być wytłumaczone jako podrzędność, nadrzędność, równoważność węzłów. Tak więc słowo byłoby określone jako lista zawierająca dwie wartości: znaczenie $\boldsymbol{m}$ i jego relację $\boldsymbol{r}$ do jakiegoś innego słowa.

\begin{equation}
(6.4.1) w = (m, r)
\end{equation}

Jak więc ta reprezentacja byłaby używana? Wyróżnilibyśmy cztery główne sieci. Sieć funkcji wartości, która korzystając z wewnętrznej reprezentacji ‘X’, zwracałaby spodziewaną wartość sytuacji opisanej przez wewnętrzną reprezentację. Ta sieć działałaby tak jak każda poprzednio opisana sieć wartości, tylko wykorzystywałaby wewnętrzną reprezentację jako wejście. Następnie sieć ‘P’ która brałaby wewnętrzną reprezentację i zwracała następne słowo, które najlepiej opisuje daną sytuację. Ta sieć byłaby siecią tworzącą kolejne słowa. Wykorzystywałaby reprezentację wewnętrzną do określania jakie słowo powinno nastąpić na następnym miejscu. Słowo składa się ze znaczenia, które jest punktem w $\boldsymbol{n}$ wymiarowej przestrzeni oraz z relacji, która określa miejsce na grafie, które powinno być wyszukiwane w następnej kolejności. Tak więc sieć ta wybierałaby najpierw punkt w \textbf{n}-dim a następnie inne, wcześniej istniejące słowo, dla którego nowe słowo będzie w relacji podrzędnej. Trzecia sieć ‘R’ podobna do funkcji reprezentacji z MuZero brałaby jako wejście listę wszystkich słów wypowiedzianych przez ‘P’ oraz odpowiadające im wartości określone przez sieć funkcji wartości i zwracałaby wewnętrzną reprezentację. Te trzy sieci byłyby trenowane łącznie. Dodatkowym wejściem do sieci funkcji wartości byłoby znaczenie najnowszego wypowiedzianego słowa, czyli punkt w \textbf{n}-dim. To sprawiałoby, że funkcja wartości wiedziałaby, z jakiego rodzaju problemem ma do czynienia. Czwartą ostatnią siecią byłaby sieć zbioru zasad. Ponieważ wskazywanie kolejnych relacji $\boldsymbol{r}$ odbywałoby się na zasadzie wskazania węzła nadrzędnego, od którego chcemy przeprowadzić rozwinięcie (co byłoby osiągnięte jako dystrybucja prawdopodobieństwa nad słowami), to musielibyśmy wybrać spośród kilku węzłów podrzędnych do rozwinięcia. W tym zadaniu pomagałaby właśnie sieć zbioru zasad. Mogłaby być ona trenowana łącznie z innymi sieciami lub też osobno. Użycie wszystkich tych sieci, jeśli wszystko zadziałałoby zgodnie z zamierzeniem, doprowadziłoby do stworzenia swego rodzaju języka, którym nauczyłby się operować nasz system.\newline

To, co zostało opisane w tym rozdziale (6.4), jest czymś przypominającym Science-Fiction. Są w nim oczywiście pewne naukowe pomysły, ale wzięte w przyszłość, po to, aby stworzyć przekonywającą historię. Autorzy nie znają żadnej implementacji pomysłów zawartych w tym rozdziale i obecnie pozostają one tylko spekulacją.

\newpage
\noindent
\textbf{Podziękowania}
\newline
Na końcu chciałbym podziękować osobom bez których ta książka nie wyglądałaby tak jak to co możecie czytać. Po pierwsze dziękuję panu Michałowi Dyzmie za przygotowanie pliku LaTeX, dzięki czemu powstała cała część techniczna, a więc odpowiednie formatowanie i wygląd tekstu. Dziękuję mu też za rady, które pomogły mi w stworzeniu tej książki. Następnie chciałbym również podziękować pani Ninie Szul za wykonanie ilustracji oraz okładki. Mam nadzieję że dzięki nim książka jest jaśniejsza oraz ładniejsza.


